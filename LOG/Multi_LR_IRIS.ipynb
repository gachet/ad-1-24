{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ5xjIxNGYesdJZUNHnLCL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gachet/ad-1-24/blob/main/LOG/Multi_LR_IRIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iPSdBgqQiiP"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.datasets  # to import iris\n",
        "import matplotlib.pyplot as plt  # %matplotlib inline # this line ÅŸs for jupyter notebooks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define your variables here!\n",
        "\n",
        "# define test datapoint\n",
        "Xt = np.array([6.4, 2.8, 5.6, 2.2])\n",
        "# define k to use in kNN\n",
        "k = 5\n",
        "# logistic regression parameters\n",
        "num_iter = 250000\n",
        "alpha = 0.1\n",
        "# #load iris dataset\n",
        "iris = sklearn.datasets.load_iris()\n",
        "# # define working data; features X and labels y\n",
        "# X = iris.data[:, :]\n",
        "# y = iris.target\n",
        "X, y = sklearn.datasets.load_iris(return_X_y=True)\n"
      ],
      "metadata": {
        "id": "uI2UVPtNQjmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 1 : Visualize data\n",
        "\n",
        "colors = ['b', 'r', 'g']\n",
        "for c in np.unique(y):\n",
        "    plt.plot(X[y == c, 0], X[y == c, 1], 'o', color=colors[int(c)])\n",
        "# also print our test datapoint\n",
        "plt.plot(Xt[0], Xt[1], '*', color=\"k\")\n",
        "plt.show()  # x0 vs x1\n",
        "\n",
        "\n",
        "# call logistic regression and print result\n",
        "log_reg_result = log_reg(X, y, alpha, num_iter, Xt)\n",
        "print(\"Test point \", Xt, \" has label \", log_reg_result, \" according to logistic regression classification\")\n",
        "print(\"which is \", iris.target_names[int(log_reg_result)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "UlwdVKedQj1z",
        "outputId": "1de270c1-a990-4c21-9df2-199239752af5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-625797232.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# also print our test datapoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "\n",
        "# define sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# define cost function\n",
        "# J(theta) = 1/m (-y^T log(h) - (1-y)^T log(1-h) )\n",
        "def cost(h, y):\n",
        "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)) / y.size\n",
        "\n",
        "# define z = theta transpose * x\n",
        "# z = np.dot(X, theta)\n",
        "# Hypothesis(x) = sigmoid(z)\n",
        "# h = sigmoid(z)\n",
        "# predict for test data\n",
        "# P (Xt | class = 1)\n",
        "def predict(Xt, theta):\n",
        "    z = np.dot(np.transpose(Xt), theta)\n",
        "    return sigmoid(z)"
      ],
      "metadata": {
        "id": "S696u8-mQj4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# gradient is the partial derivative of loss function wrt theta\n",
        "# gradient = np.dot(X.T, (h - y)) / y.size\n",
        "\n"
      ],
      "metadata": {
        "id": "nipo8d7_Qj7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_desc(X, y, alpha, num_iter):\n",
        "    # run gradient descent to adjust theta\n",
        "    #  1. Calculate gradient average\n",
        "    #  2. Multiply by learning rate alpha\n",
        "    #  3. Subtract from weights\n",
        "    # init weights - i choose all to be zero initially for consistent results\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    for i in range(num_iter):\n",
        "        z = np.dot(X, theta)\n",
        "        h = sigmoid(z)\n",
        "        gradient = np.dot(X.T, (h - y)) / y.size\n",
        "        theta = theta - alpha * gradient\n",
        "\n",
        "        # if( i % 50000 == 0):\n",
        "        #     calc_loss = cost(h, y)\n",
        "        #     print('iter num: ', i, 'cost : ', calc_loss.mean(), 'theta: ', theta)\n",
        "    print('iter num: ', i, 'cost : ', cost(h, y).mean(), 'theta: ', theta)\n",
        "    return theta\n"
      ],
      "metadata": {
        "id": "Nx9Sow_NTzZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_reg(X, y, alpha, num_iter, Xt):\n",
        "    # add intercept\n",
        "    intercept = np.ones((X.shape[0], 1))\n",
        "    X = np.concatenate((intercept, X), axis=1)\n",
        "    # add X0=1 to input data\n",
        "    Xt = np.concatenate(([1], Xt))\n",
        "\n",
        "    # build a model for each class to decide which one test data belongs to\n",
        "\n",
        "    # model for class 0\n",
        "    y_0 = np.copy(y)\n",
        "    y_0[y == 2] = 1\n",
        "    y_0 = y_0 - 1\n",
        "    y_0 = y_0 * -1\n",
        "\n",
        "    theta_0 = grad_desc(X, y_0, alpha, num_iter)\n",
        "\n",
        "    # model for class 1\n",
        "    y_1 = np.copy(y)\n",
        "    y_1[y == 2] = 0\n",
        "\n",
        "    theta_1 = grad_desc(X, y_1, alpha, num_iter)\n",
        "\n",
        "    # model for class 2\n",
        "    y_2 = np.copy(y)\n",
        "    y_2[y == 1] = 0\n",
        "    y_2[y == 2] = 1\n",
        "\n",
        "    theta_2 = grad_desc(X, y_2, alpha, num_iter)\n",
        "    # use i th model to decide for c_i\n",
        "\n",
        "    preds = np.zeros(len(np.unique(y)))\n",
        "    preds[0] = predict(Xt, theta_0)\n",
        "    preds[1] = predict(Xt, theta_1)\n",
        "    preds[2] = predict(Xt, theta_2)\n",
        "\n",
        "    print(\"class 0 P = \", preds[0])\n",
        "    print(\"class 1 P = \", preds[1])\n",
        "    print(\"class 2 P = \", preds[2])\n",
        "\n",
        "    # choose max\n",
        "    class_label = np.where(preds == max(preds))\n",
        "\n",
        "    # print result\n",
        "    # print(\"Test point \", Xt, \" has label \", class_label[0], \" according to logistic regression classification\")\n",
        "\n",
        "    return class_label[0]\n"
      ],
      "metadata": {
        "id": "BS8A-YD9QkAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rxa2Fe7HQkDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-1Q_UH7IQkFt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}